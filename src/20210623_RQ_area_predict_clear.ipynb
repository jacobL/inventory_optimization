{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob,os\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tqdm\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, GlobalMaxPool1D, Layer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b = True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        \n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis = -1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm = [0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        \n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm = [0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "def gelu(x):\n",
    "    \"\"\"Gaussian Error Linear Unit.\n",
    "    This is a smoother version of the RELU.\n",
    "    Original paper: https://arxiv.org/abs/1606.08415\n",
    "    refer : https://github.com/google-research/bert/blob/bee6030e31e42a9394ac567da170a89a98d2062f/modeling.py#L264\n",
    "    Args:\n",
    "        x: float Tensor to perform activation.\n",
    "    Returns:\n",
    "        `x` with the GELU activation applied.\n",
    "    \"\"\"\n",
    "    cdf = 0.5 * (1.0 + tf.tanh(\n",
    "        (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "    return x * cdf\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    \n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation = gelu),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dff, rate = 0.1):\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training = training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training = training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, rate = 0.1):\n",
    "        \n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.rate = rate\n",
    "\n",
    "        self.embedding = tf.keras.layers.Dense(self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(self.d_model, self.num_heads, self.dff, self.rate) \n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(self.rate)\n",
    "        \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'num_layers': self.num_layers,\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dff': self.dff,\n",
    "            'dropout': self.dropout,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, x, training, mask = None):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = self.dropout(x, training = training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "def create_Transformer(num_columns, num_layers, d_model, num_heads, dff, dropout_rate):\n",
    "    # d_model: Embedding depth of the model.\n",
    "    # num_heads: Number of heads for Multi-head attention. d_model % num_heads = 0\n",
    "    # dff: Depth of the point wise feed-forward network\n",
    "    \n",
    "    inp = tf.keras.layers.Input(shape = (num_columns, ))\n",
    "    x = tf.keras.layers.Reshape((1, num_columns))(inp)\n",
    "    \n",
    "    x = TransformerEncoder(num_layers, d_model, num_heads, dff, dropout_rate)(x)[:, 0, :]\n",
    "            \n",
    "    out = tfa.layers.WeightNormalization(tf.keras.layers.Dense(1, activation = 'sigmoid'))(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs = inp, outputs = out)\n",
    "    \n",
    "    model.compile(loss=\"mae\", optimizer=\"adam\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file(path):                \n",
    "    files =os.listdir(path)\n",
    "    files.sort() \n",
    "    data_list= []\n",
    "    for file in files:\n",
    "        if not  os.path.isdir(path +file):     \n",
    "            f_name = str(file)        \n",
    "            tr = '\\\\'   \n",
    "            filename = path + tr + f_name        \n",
    "            data_list.append(filename)  \n",
    "    return data_list \n",
    "\n",
    "def data_concat(path):\n",
    "    data_list = get_file(path)\n",
    "    l = []\n",
    "    ll=[]\n",
    "    n = []\n",
    "    df_all=pd.DataFrame()\n",
    "\n",
    "    for j in range(len(data_list)):\n",
    "        print(data_list[j])\n",
    "        df = pd.read_excel(data_list[j])\n",
    "        ll.append(len(df))\n",
    "        l.append([len(df),j])\n",
    "        df=pd.DataFrame(df)\n",
    "        df_all=pd.concat([df_all,df],axis=0)\n",
    "\n",
    "    df_all = df_all.reset_index(drop=True)\n",
    "    df_all_recol=df_all[['GROUP_CODE','MODEL_NAME','ISSUE_DATE','RC_ID','FAILURE_STAGE','PRODUCT_ID']]\n",
    "    print(df_all_recol.shape)\n",
    "    return df_all_recol\n",
    "\n",
    "# df_all_recol.to_csv('df_all_recol.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\RQ售後庫存資源分配\\rawdata_new\\RQ_L3_RAWDATA_ISSUE_DATE_202103_20210403.xlsx\n",
      "D:\\RQ售後庫存資源分配\\rawdata_new\\RQ_L3_RAWDATA_ISSUE_DATE_202104_20210503.xlsx\n",
      "D:\\RQ售後庫存資源分配\\rawdata_new\\RQ_L3_RAWDATA_ISSUE_DATE_202105_20210603.xlsx\n",
      "(136264, 6)\n"
     ]
    }
   ],
   "source": [
    "path = r'D:\\RQ售後庫存資源分配\\rawdata_new'\n",
    "df_all_recol_new = data_concat(path)\n",
    "df_all_recol=pd.read_csv('df_all_recol.csv')\n",
    "df_all_recol_all = pd.concat([df_all_recol, df_all_recol_new],axis=0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_recol = df_all_recol_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = r'D:\\RQ售後庫存資源分配'\n",
    "RC_ID_dict = pd.read_excel(path+'\\\\all_data(1).xlsx',sheet_name='RC_ID對應的地區').dropna(axis=1, thresh=3)\n",
    "RC_ID_mapping = pd.read_excel(path+'\\\\all_data(1).xlsx',sheet_name='RC_ID mapping')\n",
    "APPLICATION_dict = pd.read_excel(path+'\\\\all_data(1).xlsx',sheet_name='APPLICATION').iloc[:,:2]\n",
    "RC_keep = RC_ID_dict[(RC_ID_dict['RC ID_分類表']!='虛擬_RC')&(RC_ID_dict['RC ID_分類表']!='異常大量退返')]['RC_ID']\n",
    "APPLICATION_dict_keep = APPLICATION_dict['Code']\n",
    "\n",
    "\n",
    "df_all_recol.replace(list(RC_ID_mapping['轉換前']), list(RC_ID_mapping['轉換後']), inplace=True)\n",
    "df_all_recol = df_all_recol[(df_all_recol.PRODUCT_ID!='FFFFFFFFFFFFF')&\n",
    "                            (df_all_recol.RC_ID.isin(RC_keep))].reset_index(drop=True)\n",
    "df_all_recol = df_all_recol[['GROUP_CODE','ISSUE_DATE','RC_ID','FAILURE_STAGE','PRODUCT_ID']].dropna()\n",
    "df_all_recol.loc[df_all_recol['FAILURE_STAGE']=='FR','FS']='FR'\n",
    "df_all_recol.loc[df_all_recol['FAILURE_STAGE']!='FR','FS']='LR'\n",
    "df_all_recol['ISSUE_DATE'] = pd.to_datetime(df_all_recol['ISSUE_DATE'], format='%Y-%m-%d')\n",
    "df_all_recol['year_month'] = df_all_recol['ISSUE_DATE'].dt.strftime('%Y-%m')\n",
    "df_all_recol['count']=1\n",
    "df_all_recol_ASUS = df_all_recol.copy()\n",
    "ASUS_LR = df_all_recol_ASUS[['GROUP_CODE', 'RC_ID', 'FAILURE_STAGE', 'PRODUCT_ID',\n",
    "       'FS', 'year_month', 'count']]\n",
    "df_ASUS_pivot = ASUS_LR.pivot_table(values='count', index='year_month',\n",
    "                                    columns=['GROUP_CODE','FS','PRODUCT_ID','RC_ID'], aggfunc='count')\n",
    "df_ASUS_all_pivot = ASUS_LR.pivot_table(values='count', index='year_month',\n",
    "                                        columns=['GROUP_CODE','FS','PRODUCT_ID'], aggfunc='count')\n",
    "df_ASUS_pivot.fillna(0,inplace=True)\n",
    "df_ASUS_all_pivot.fillna(0,inplace=True)\n",
    "df_ASUS_pivot_rollsum = df_ASUS_pivot.rolling(4).sum()\n",
    "df_ASUS_all_pivot_rollsum = df_ASUS_all_pivot.rolling(4).sum()\n",
    "\n",
    "df_pivot_v1 = (df_ASUS_pivot_rollsum/df_ASUS_all_pivot_rollsum).iloc[3:,:].transpose().reset_index()\n",
    "dt_col = df_pivot_v1.columns[df_pivot_v1.columns.str.contains('20')]\n",
    "df_pivot_v1[dt_col[0]]=df_pivot_v1[dt_col[0]].fillna(0)\n",
    "df_pivot_v1[dt_col] = df_pivot_v1[dt_col].transpose().fillna(method='ffill').transpose()\n",
    "train = df_pivot_v1.drop(columns=['PRODUCT_ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((127048, 14), (15881, 14), (15881, 14))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_col = train.columns[train.columns.str.contains('20')]\n",
    "train_col=['FS']\n",
    "train_col.extend(month_col)\n",
    "train=train[train_col]\n",
    "feature_col = train.columns[~train.columns.str.contains('20')]\n",
    "\n",
    "data_monthly = train[month_col]\n",
    "\n",
    "fix = train[feature_col]\n",
    "step=12\n",
    "data_stack=pd.DataFrame()\n",
    "creat_col=['t_11','t_10','t_9','t_8','t_7','t_6','t_5','t_4','t_3','t_2','t_1','t','y']\n",
    "creat_col=creat_col[-(step+1):]\n",
    "\n",
    "for i in range(data_monthly.shape[1]-step):\n",
    "#     print(i)\n",
    "    data_12 = pd.concat([fix,data_monthly.iloc[:,0+i:step+1+i]],axis=1)\n",
    "    data_12.columns=list(feature_col)+creat_col\n",
    "    data_stack=pd.concat([data_stack,data_12])\n",
    "\n",
    "data_stack=pd.get_dummies(data_stack,dtype='int64')\n",
    "\n",
    "recent_data = data_stack.iloc[-(train.shape[0]):,1:]\n",
    "train_x=data_stack.iloc[:-(train.shape[0])].drop(['y'],axis=1)\n",
    "train_y=data_stack.iloc[:-(train.shape[0])].y\n",
    "chg_x=data_stack.iloc[-(train.shape[0]):].drop(['y'],axis=1)\n",
    "chg_y=data_stack.iloc[-(train.shape[0]):].y\n",
    "\n",
    "train_x.shape, chg_x.shape, recent_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape : (101638, 14)\n",
      "X_test Shape : (25410, 14)\n",
      "y_train Shape : (101638,)\n",
      "y_test Shape : (25410,)\n"
     ]
    }
   ],
   "source": [
    "X_std = train_x.copy()\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_std,train_y,test_size=0.2, random_state=77)\n",
    "print('X_train Shape :',X_train.shape)\n",
    "print('X_test Shape :',X_test.shape)\n",
    "print('y_train Shape :',y_train.shape)\n",
    "print('y_test Shape :',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((101638, 14), (25410, 14), (101638,), (25410,))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 14)]              0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 1, 14)             0         \n",
      "_________________________________________________________________\n",
      "transformer_encoder_3 (Trans (None, None, 16)          5968      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice_3  [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "weight_normalization_3 (Weig (None, 1)                 36        \n",
      "=================================================================\n",
      "Total params: 6,004\n",
      "Trainable params: 5,986\n",
      "Non-trainable params: 18\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# - num_layers: 決定要有幾個 EncoderLayers, 前面影片中的 `N`\n",
    "\n",
    "model = create_Transformer(X_std.shape[1], num_layers=4, d_model=16, num_heads=8, dff=8, dropout_rate=0.1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to tf.Tensor(0.0025, shape=(), dtype=float32).\n",
      "Epoch 1/500\n",
      "1589/1589 [==============================] - 6s 4ms/step - loss: 0.0717 - val_loss: 0.0551\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to tf.Tensor(0.0025, shape=(), dtype=float32).\n",
      "Epoch 2/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0540 - val_loss: 0.0494\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to tf.Tensor(0.0025, shape=(), dtype=float32).\n",
      "Epoch 3/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0525 - val_loss: 0.0508\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to tf.Tensor(0.0025, shape=(), dtype=float32).\n",
      "Epoch 4/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0520 - val_loss: 0.0523\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to tf.Tensor(0.0025, shape=(), dtype=float32).\n",
      "Epoch 5/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0522 - val_loss: 0.0491\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to tf.Tensor(0.0025, shape=(), dtype=float32).\n",
      "Epoch 6/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0513 - val_loss: 0.0495\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to tf.Tensor(0.0025, shape=(), dtype=float32).\n",
      "Epoch 7/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0503 - val_loss: 0.0482\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to tf.Tensor(0.0025, shape=(), dtype=float32).\n",
      "Epoch 8/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0514 - val_loss: 0.0491\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to tf.Tensor(0.0025, shape=(), dtype=float32).\n",
      "Epoch 9/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0500 - val_loss: 0.0477\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to tf.Tensor(0.0025, shape=(), dtype=float32).\n",
      "Epoch 10/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0504 - val_loss: 0.0537\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to tf.Tensor(0.0025, shape=(), dtype=float32).\n",
      "Epoch 11/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0494 - val_loss: 0.0495\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to tf.Tensor(0.0025, shape=(), dtype=float32).\n",
      "Epoch 12/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0498 - val_loss: 0.0474\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to tf.Tensor(0.0025, shape=(), dtype=float32).\n",
      "Epoch 13/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0493 - val_loss: 0.0479\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to tf.Tensor(0.0025, shape=(), dtype=float32).\n",
      "Epoch 14/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0494 - val_loss: 0.0468\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005, shape=(), dtype=float32).\n",
      "Epoch 15/500\n",
      "1589/1589 [==============================] - 6s 3ms/step - loss: 0.0475 - val_loss: 0.0460\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005, shape=(), dtype=float32).\n",
      "Epoch 16/500\n",
      "1589/1589 [==============================] - 6s 4ms/step - loss: 0.0474 - val_loss: 0.0462\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005, shape=(), dtype=float32).\n",
      "Epoch 17/500\n",
      "1589/1589 [==============================] - 6s 4ms/step - loss: 0.0473 - val_loss: 0.0461\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005, shape=(), dtype=float32).\n",
      "Epoch 18/500\n",
      "1589/1589 [==============================] - 6s 4ms/step - loss: 0.0471 - val_loss: 0.0459\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005, shape=(), dtype=float32).\n",
      "Epoch 19/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0472 - val_loss: 0.0457\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005, shape=(), dtype=float32).\n",
      "Epoch 20/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0471 - val_loss: 0.0461\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005, shape=(), dtype=float32).\n",
      "Epoch 21/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0470 - val_loss: 0.0456\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005, shape=(), dtype=float32).\n",
      "Epoch 22/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0469 - val_loss: 0.0467\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005, shape=(), dtype=float32).\n",
      "Epoch 23/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0470 - val_loss: 0.0464\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005, shape=(), dtype=float32).\n",
      "Epoch 24/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0469 - val_loss: 0.0458\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005, shape=(), dtype=float32).\n",
      "Epoch 25/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0469 - val_loss: 0.0458\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005, shape=(), dtype=float32).\n",
      "Epoch 26/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0468 - val_loss: 0.0458\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005, shape=(), dtype=float32).\n",
      "Epoch 27/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0468 - val_loss: 0.0464\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005, shape=(), dtype=float32).\n",
      "Epoch 28/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0468 - val_loss: 0.0457\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005, shape=(), dtype=float32).\n",
      "Epoch 29/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0467 - val_loss: 0.0458\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to tf.Tensor(0.000100000005, shape=(), dtype=float32).\n",
      "Epoch 30/500\n",
      "1589/1589 [==============================] - 6s 3ms/step - loss: 0.0466 - val_loss: 0.0457\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to tf.Tensor(0.000100000005, shape=(), dtype=float32).\n",
      "Epoch 31/500\n",
      "1589/1589 [==============================] - 6s 3ms/step - loss: 0.0465 - val_loss: 0.0455\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to tf.Tensor(0.000100000005, shape=(), dtype=float32).\n",
      "Epoch 32/500\n",
      "1589/1589 [==============================] - 6s 3ms/step - loss: 0.0464 - val_loss: 0.0455\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to tf.Tensor(0.000100000005, shape=(), dtype=float32).\n",
      "Epoch 33/500\n",
      "1589/1589 [==============================] - 6s 3ms/step - loss: 0.0465 - val_loss: 0.0454\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to tf.Tensor(0.000100000005, shape=(), dtype=float32).\n",
      "Epoch 34/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0464 - val_loss: 0.0453\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to tf.Tensor(0.000100000005, shape=(), dtype=float32).\n",
      "Epoch 35/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0464 - val_loss: 0.0452\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to tf.Tensor(0.000100000005, shape=(), dtype=float32).\n",
      "Epoch 36/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0463 - val_loss: 0.0454\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to tf.Tensor(0.000100000005, shape=(), dtype=float32).\n",
      "Epoch 37/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0464 - val_loss: 0.0452\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to tf.Tensor(0.000100000005, shape=(), dtype=float32).\n",
      "Epoch 38/500\n",
      "1589/1589 [==============================] - 6s 4ms/step - loss: 0.0464 - val_loss: 0.0455\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to tf.Tensor(0.000100000005, shape=(), dtype=float32).\n",
      "Epoch 39/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1589/1589 [==============================] - 6s 3ms/step - loss: 0.0464 - val_loss: 0.0454\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to tf.Tensor(0.000100000005, shape=(), dtype=float32).\n",
      "Epoch 40/500\n",
      "1589/1589 [==============================] - 6s 4ms/step - loss: 0.0464 - val_loss: 0.0453\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to tf.Tensor(0.000100000005, shape=(), dtype=float32).\n",
      "Epoch 41/500\n",
      "1589/1589 [==============================] - 6s 4ms/step - loss: 0.0464 - val_loss: 0.0455\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to tf.Tensor(0.000100000005, shape=(), dtype=float32).\n",
      "Epoch 42/500\n",
      "1589/1589 [==============================] - 6s 4ms/step - loss: 0.0463 - val_loss: 0.0454\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to tf.Tensor(0.000100000005, shape=(), dtype=float32).\n",
      "Epoch 43/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0463 - val_loss: 0.0453\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to tf.Tensor(0.000100000005, shape=(), dtype=float32).\n",
      "Epoch 44/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0464 - val_loss: 0.0459\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to tf.Tensor(2.0000001e-05, shape=(), dtype=float32).\n",
      "Epoch 45/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0463 - val_loss: 0.0453\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to tf.Tensor(2.0000001e-05, shape=(), dtype=float32).\n",
      "Epoch 46/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0463 - val_loss: 0.0454\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to tf.Tensor(2.0000001e-05, shape=(), dtype=float32).\n",
      "Epoch 47/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0463 - val_loss: 0.0454\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to tf.Tensor(2.0000001e-05, shape=(), dtype=float32).\n",
      "Epoch 48/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0463 - val_loss: 0.0453\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to tf.Tensor(2.0000001e-05, shape=(), dtype=float32).\n",
      "Epoch 49/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0463 - val_loss: 0.0453\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to tf.Tensor(2.0000001e-05, shape=(), dtype=float32).\n",
      "Epoch 50/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0463 - val_loss: 0.0453\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to tf.Tensor(2.0000001e-05, shape=(), dtype=float32).\n",
      "Epoch 51/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0463 - val_loss: 0.0453\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to tf.Tensor(2.0000001e-05, shape=(), dtype=float32).\n",
      "Epoch 52/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0463 - val_loss: 0.0453\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to tf.Tensor(2.0000001e-05, shape=(), dtype=float32).\n",
      "Epoch 53/500\n",
      "1589/1589 [==============================] - 6s 4ms/step - loss: 0.0463 - val_loss: 0.0453\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to tf.Tensor(2.0000001e-05, shape=(), dtype=float32).\n",
      "Epoch 54/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0463 - val_loss: 0.0453\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to tf.Tensor(2.0000001e-05, shape=(), dtype=float32).\n",
      "Epoch 55/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0463 - val_loss: 0.0454\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to tf.Tensor(2.0000001e-05, shape=(), dtype=float32).\n",
      "Epoch 56/500\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0463 - val_loss: 0.0454\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to tf.Tensor(2.0000001e-05, shape=(), dtype=float32).\n",
      "Epoch 57/500\n",
      "1583/1589 [============================>.] - ETA: 0s - loss: 0.0463Restoring model weights from the end of the best epoch.\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.0463 - val_loss: 0.0453\n",
      "Epoch 00057: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.0025 # 初始化學習率\n",
    "    drop = 0.2 # 每次遞減率\n",
    "    epochs_drop = 15 # 每 10 個 epochs 降低一次學習率\n",
    "    lrate = initial_lrate * tf.math.pow(drop, tf.math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lr_schedule = LearningRateScheduler(step_decay, verbose=1)\n",
    "callback = EarlyStopping(monitor=\"val_loss\", patience=20, verbose=1, mode=\"auto\", restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 500, batch_size = 64, \n",
    "                    callbacks = [callback, lr_schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1dd8f095888>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax.plot(history.history['val_loss'], color='r', label=\"Validation loss\")\n",
    "ax.legend(loc='best', shadow=True, fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(\"RQ_transformer_ffill_v1.h5\")\n",
    "# model.load_weights(\"RQ_transformer_ffill_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------all----------\n",
      "R-sq: 0.8673\n",
      "MAE: 0.045\n",
      "----------test----------\n",
      "R-sq: 0.8656\n",
      "MAE: 0.045\n",
      "----------last----------\n",
      "R-sq: 0.8916\n",
      "MAE: 0.037\n"
     ]
    }
   ],
   "source": [
    "# all\n",
    "y_head=model.predict(X_std)\n",
    "print('-'*10+'all'+'-'*10)\n",
    "print('R-sq: %.4f'%(r2_score(train_y,y_head)))\n",
    "print('MAE: %.3f'%(mean_absolute_error(train_y,y_head)))\n",
    "\n",
    "# test\n",
    "y_head=model.predict(X_test)\n",
    "print('-'*10+'test'+'-'*10)\n",
    "print('R-sq: %.4f'%(r2_score(y_test,y_head)))\n",
    "print('MAE: %.3f'%(mean_absolute_error(y_test,y_head)))\n",
    "\n",
    "# last month\n",
    "y_head=model.predict(chg_x)\n",
    "print('-'*10+'last'+'-'*10)\n",
    "print('R-sq: %.4f'%(r2_score(chg_y,y_head)))\n",
    "print('MAE: %.3f'%(mean_absolute_error(chg_y,y_head)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pred(df_pivot_v1, y_head, NewColName='NewMonth'):\n",
    "    pred_lastmonth = df_pivot_v1.loc[:,:'RC_ID']\n",
    "    pred_lastmonth[NewColName]=y_head\n",
    "    pred_lastmonth[NewColName] = round(pred_lastmonth[NewColName],1)\n",
    "    return pred_lastmonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_lastmonth = model_pred(df_pivot_v1, model.predict(chg_x), NewColName=dt_col[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>year_month</th>\n",
       "      <th>GROUP_CODE</th>\n",
       "      <th>FS</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>RC_ID</th>\n",
       "      <th>2021-06</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A NOVO</td>\n",
       "      <td>FR</td>\n",
       "      <td>GM215HCA3GE0S</td>\n",
       "      <td>PCZ</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A NOVO</td>\n",
       "      <td>FR</td>\n",
       "      <td>GM215HCA3GF0S</td>\n",
       "      <td>PCZ</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A NOVO</td>\n",
       "      <td>FR</td>\n",
       "      <td>GM215HCA3GL0S</td>\n",
       "      <td>PCZ</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A NOVO</td>\n",
       "      <td>FR</td>\n",
       "      <td>GM215HCA3GU0S</td>\n",
       "      <td>PCZ</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A NOVO</td>\n",
       "      <td>FR</td>\n",
       "      <td>GM215HGE2P20S</td>\n",
       "      <td>PCZ</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15876</th>\n",
       "      <td>高照</td>\n",
       "      <td>LR</td>\n",
       "      <td>GS400HJ6E810S</td>\n",
       "      <td>HLM</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15877</th>\n",
       "      <td>高照</td>\n",
       "      <td>LR</td>\n",
       "      <td>GS400HJ6E820S</td>\n",
       "      <td>HLM</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15878</th>\n",
       "      <td>龍亭</td>\n",
       "      <td>LR</td>\n",
       "      <td>GS231AJ10140S</td>\n",
       "      <td>HL2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15879</th>\n",
       "      <td>龍亭</td>\n",
       "      <td>LR</td>\n",
       "      <td>GS350AJ10100S</td>\n",
       "      <td>HL2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15880</th>\n",
       "      <td>龍亭</td>\n",
       "      <td>LR</td>\n",
       "      <td>GS350AJ10110S</td>\n",
       "      <td>HL2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15881 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "year_month GROUP_CODE  FS     PRODUCT_ID RC_ID  2021-06\n",
       "0              A NOVO  FR  GM215HCA3GE0S   PCZ      1.0\n",
       "1              A NOVO  FR  GM215HCA3GF0S   PCZ      1.0\n",
       "2              A NOVO  FR  GM215HCA3GL0S   PCZ      1.0\n",
       "3              A NOVO  FR  GM215HCA3GU0S   PCZ      1.0\n",
       "4              A NOVO  FR  GM215HGE2P20S   PCZ      1.0\n",
       "...               ...  ..            ...   ...      ...\n",
       "15876              高照  LR  GS400HJ6E810S   HLM      1.0\n",
       "15877              高照  LR  GS400HJ6E820S   HLM      1.0\n",
       "15878              龍亭  LR  GS231AJ10140S   HL2      1.0\n",
       "15879              龍亭  LR  GS350AJ10100S   HL2      1.0\n",
       "15880              龍亭  LR  GS350AJ10110S   HL2      1.0\n",
       "\n",
       "[15881 rows x 5 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "new_month = dt.datetime.strptime(dt_col[-1], \"%Y-%m\")+relativedelta(months=1)\n",
    "new_monthName = new_month.strftime(\"%Y-%m\")\n",
    "PredNewMonth = model_pred(df_pivot_v1, model.predict(recent_data), NewColName=new_monthName)\n",
    "PredNewMonth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
